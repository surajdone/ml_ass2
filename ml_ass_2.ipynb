{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "413a36c8-a994-49db-91b1-0dedc55994e5",
   "metadata": {},
   "source": [
    "## Q1: Define overfitting and underfitting in machine learning. What are the consequences of each, and howcan they be mitigated?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a2a3693c-e27e-4e53-a8aa-a052cbb3562a",
   "metadata": {},
   "source": [
    "overfitting is occur when machine or model are too closely with the trained data with noise and fluctuations model performing on test data more ccuretly more than 90%\n",
    "but when new values or new data provide to the data it not give the accuraccy that give like trained data it means machine familier with the trained data but not work properly on the \n",
    "new data its accuraccy will be down\n",
    "\n",
    "Regularization: Regularization techniques like L1 and L2 regularization can be applied to penalize complex models, reducing overfitting.\n",
    "Cross-validation: Use techniques like k-fold cross-validation to evaluate your model's performance on different subsets of data, helping you detect and mitigate overfitting.\n",
    "Simplify the model: Reduce model complexity by using fewer features, a smaller network architecture (in the case of neural networks), or limiting the maximum depth of decision trees.\n",
    "\n",
    "Underfitting-\n",
    "Underfitting occurs when a machine learning model is too simple to capture the underlying patterns in the data.\n",
    "Consequences: Underfit models perform poorly on both the training data and unseen data because they fail to capture the complexity of the underlying relationships.\n",
    "\n",
    "ncrease model complexity: Use a more sophisticated model or increase its capacity by adding more layers or neurons (in the case of neural networks).\n",
    "Feature engineering: Create more relevant features or transform existing features to make the data more suitable for the model.\n",
    "Collect more data: Sometimes, underfitting can be mitigated by gathering additional data to provide more information to the model.\n",
    "Hyperparameter tuning: Adjust hyperparameters (e.g., learning rate, batch size) to fine-tune the model's performance.\n",
    "Ensemble methods: Combine multiple simple models (e.g., decision trees) to create a more powerful ensemble model that can capture complex patterns.\n",
    "Finding the right balance between overfitting and underfitting is often an iterative process, and it requires a deep understanding of the problem, the data, and the chosen machine learning algorithm. Techniques like cross-validation and monitoring performance on validation sets are essential for achieving a well-generalized model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649eabfe-9c3e-495f-b279-2155d1a60684",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c91e5d7a-f98f-495b-959b-85514a6f2638",
   "metadata": {},
   "source": [
    "## Q2: How can we reduce overfitting? Explain in brief."
   ]
  },
  {
   "cell_type": "raw",
   "id": "f35edea8-7522-4062-9eb6-1babc5c17443",
   "metadata": {},
   "source": [
    "we can reduce overfitting by incresing the trainning dada\n",
    "DROPOUT\n",
    "Cross-Validation: Use techniques like k-fold cross-validation to assess the model's performance on different subsets of the training data. This helps detect overfitting by evaluating how well the model generalizes to unseen data.\n",
    "\n",
    "Feature Selection: Carefully choose and engineer relevant features while discarding irrelevant or redundant ones. Reducing the dimensionality of the data can help prevent overfitting.\n",
    "Simplifying the Model: Reduce the complexity of the model by using fewer layers, nodes, or parameters. For example, in deep learning, you can decrease the number of hidden layers or neurons in a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e10e2b-ed60-4de2-89c1-78fa0c309727",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "30ecd23e-fa28-4737-b604-8b74504629a4",
   "metadata": {},
   "source": [
    "## Q3: Explain underfitting. List scenarios where underfitting can occur in ML."
   ]
  },
  {
   "cell_type": "raw",
   "id": "0d84274f-3d6c-40db-872c-faf7de6106e2",
   "metadata": {},
   "source": [
    "UNDERFITTING OCCUR when the model not accurte as trained data and the new data its accuracy is down as expected\n",
    "Underfitting happens when a machine learning model is overly simplistic, lacking the capacity to represent the relationships within the data accurately. The model essentially \"underfits\" the training data by making overly generalized assumptions. As a result, it performs poorly because it can't capture the inherent complexity in the data.\n",
    "1)when small amoun of data privide to model to train\n",
    "2)unsufficient data\n",
    "3)ignoring interactions\n",
    "4)small training dataset\n",
    "5)bais in data sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9840a394-acf1-48bf-91e7-8340040a4ccc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9652f5ae-d2ad-42bf-a371-d880e5beb8c6",
   "metadata": {},
   "source": [
    "## Q4: Explain the bias-variance tradeoff in machine learning. What is the relationship between bias andvariance, and how do they affect model performance?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "046032d4-5fb8-43a6-8407-b1a481fd88e0",
   "metadata": {},
   "source": [
    "Bias refers to the error introduced by approximating a real-world problem, which may be complex, by a simplified model. It represents the model's tendency to make systematic errors consistently.\n",
    "vA high-bias model is overly simplistic and makes strong assumptions about the data, often resulting in underfitting. It cannot capture the underlying patterns in the data.\n",
    "Example: Fitting a linear regression model to data with a nonlinear relationship would introduce bias because the model cannot represent the true data distribution.\n",
    "\n",
    "Variance refers to the model's sensitivity to small fluctuations or noise in the training data. It measures the model's tendency to fit the training data closely, including the random noise, rather than just the underlying patterns.\n",
    "A high-variance model is overly complex and fits the training data too closely, often resulting in overfitting. It does not generalize well to unseen data.\n",
    "Example: A high-degree polynomial regression model that fits the training data precisely but fails to generalize to new data points because it captures noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9090ac-0623-47d3-95c3-f341cd885071",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "10668d54-2a26-459c-a12a-c27eb84483cc",
   "metadata": {},
   "source": [
    "## Discuss some common methods for detecting overfitting and underfitting in machine learning models.How can you determine whether your model is overfitting or underfitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01153255-0df7-40a2-9d85-e63fe455cdaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "6d89aa68-8225-43b6-a6af-7c459c62bd3f",
   "metadata": {},
   "source": [
    "Overfitting: In a validation curve, as the model complexity (e.g., polynomial degree, depth of a decision tree) increases, the training error tends to decrease, but the validation error increases. If you observe a growing gap between training and validation errors, it's a sign of overfitting.\n",
    "Underfitting: Both training and validation errors are high and plateau, indicating that the model is too simple to capture the underlying patterns.\n",
    "\n",
    "learning curve \n",
    "overfitting is detects that model working and accuraccy that found diffrence between the training and validation\n",
    "that model work better on training data but the performance will be poor on unseen data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26f23f6-3cbb-4dd7-be55-6e16e445ca08",
   "metadata": {},
   "source": [
    "## Q6: Compare and contrast bias and variance in machine learning. What are some examples of high biasand high variance models, and how do they differ in terms of their performance?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "627ec907-5566-42d4-8c2f-5bc95bc311be",
   "metadata": {},
   "source": [
    "Bais reafers to the real world problems that occurs with the machines it represent to model tendecy to make error constantly\n",
    "Characteristics: A high-bias model is overly simplistic and makes strong assumptions about the data, often resulting in underfitting.\n",
    "Performance: It performs poorly on both the training data and unseen data because it cannot capture the underlying patterns.\n",
    "\n",
    "High Variance (Overfitting):\n",
    "\n",
    "Characteristics: A high-variance model is overly complex and fits the training data too closely, often resulting in overfitting.\n",
    "Performance: It performs very well on the training data but poorly on unseen data because it captures noise in the training data.\n",
    "Examples:\n",
    "High-degree polynomial regression models that fit training data precisely but fail to generalize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63d9f03-338f-4203-8202-c20dabcf2912",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bca6fa-595c-4a8c-87ed-d528f772d23f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "4c865c75-1a60-4e14-ac84-158b322a141d",
   "metadata": {},
   "source": [
    "Regularization is a set of techniques in machine learning designed to prevent overfitting by adding a penalty or constraint to the model's learning process. The primary purpose of regularization is to encourage the model to have simpler, more generalized representations, reducing its sensitivity to noise in the training data. Here are some common regularization techniques and how they work:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
